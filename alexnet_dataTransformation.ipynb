{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Data Transformation (or data augmentation) as per the AlexNet paper** ##\n",
    "\n",
    "data augmentation (only image translation and horizontal reflection; read section 4.1 \n",
    "on the paper)\n",
    "- Random crop of 224x224 from the 256x256 image\n",
    "- Random horizontal flipping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Define dataset paths\n",
    "\n",
    "dataset_root = 'alexnet_dataset'\n",
    "\n",
    "# dataset_root = 'alexnet_dataset'\n",
    "train_dir = os.path.join(dataset_root, 'train')\n",
    "val_dir = os.path.join(dataset_root, 'val')\n",
    "test_dir = os.path.join(dataset_root, 'test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Why Class mapping? ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load class mapping\n",
    "class_to_idx = {}\n",
    "with open(os.path.join(dataset_root, 'class_mapping.txt'), 'r') as f:\n",
    "    for line in f:\n",
    "        cls, idx = line.strip().split(',')\n",
    "        class_to_idx[cls] = int(idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Transformation for datasets ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data transformation for training\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomCrop(224),  # Random crop to 224x224\n",
    "    transforms.RandomHorizontalFlip(),  # Random horizontal flip\n",
    "    transforms.ToTensor(),  # Convert to tensor (scales to [0, 1])\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),  # ImageNet normalization\n",
    "\n",
    "])\n",
    "\n",
    "# Data transformation for validation/testing (no augmentation)\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.CenterCrop(224),  # Center crop to 224x224\n",
    "    transforms.ToTensor(),  # Convert to tensor\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # ImageNet normalization\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: 30000 images\n",
      "Validation set: 10000 images\n",
      "Testing set: 10000 images\n",
      "Number of classes: 100\n",
      "Batch size: 128\n",
      "Data preparation complete and ready for AlexNet model training!\n"
     ]
    }
   ],
   "source": [
    "# Custom Dataset class\n",
    "class AlexNetDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.samples = []\n",
    "        \n",
    "        # Scan directory for images and labels\n",
    "        class_dirs = [d for d in os.listdir(root_dir) if os.path.isdir(os.path.join(root_dir, d))]\n",
    "        for class_name in class_dirs:\n",
    "            class_idx = class_to_idx[class_name]\n",
    "            class_dir = os.path.join(root_dir, class_name)\n",
    "            for img_name in os.listdir(class_dir):\n",
    "                if img_name.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
    "                    self.samples.append((os.path.join(class_dir, img_name), class_idx))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        try:\n",
    "            img_path, label = self.samples[idx]\n",
    "            image = Image.open(img_path).convert('RGB')\n",
    "            \n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "                \n",
    "            return image, label\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image {idx}: {e}\")\n",
    "            # Return a placeholder instead of failing completely\n",
    "            # This helps prevent worker crashes\n",
    "            placeholder = torch.zeros((3, 224, 224)) if self.transform else Image.new('RGB', (224, 224))\n",
    "            return placeholder, 0  # Return placeholder with dummy label\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = AlexNetDataset(train_dir, transform=train_transform)\n",
    "val_dataset = AlexNetDataset(val_dir, transform=val_transform)\n",
    "test_dataset = AlexNetDataset(test_dir, transform=val_transform)\n",
    "\n",
    "# Create data loaders\n",
    "# AlexNet paper used a batch size of 128\n",
    "batch_size = 128\n",
    "# batch_size = 16\n",
    "# train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=batch_size, \n",
    "    shuffle=True,  \n",
    "    num_workers=0,\n",
    "    pin_memory=True if torch.cuda.is_available() else False,\n",
    "    # persistent_workers=False\n",
    "    )\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, \n",
    "    batch_size=batch_size, \n",
    "    shuffle=False, \n",
    "    num_workers=0, \n",
    "    pin_memory=True if torch.cuda.is_available() else False,\n",
    "    # persistent_workers=False\n",
    "    )\n",
    "test_loader = DataLoader(\n",
    "    test_dataset, \n",
    "    batch_size=batch_size, \n",
    "    shuffle=False, \n",
    "    num_workers=0, \n",
    "    pin_memory=True if torch.cuda.is_available() else False,\n",
    "    persistent_workers=False\n",
    "    )\n",
    "\n",
    "print(f\"Training set: {len(train_dataset)} images\")\n",
    "print(f\"Validation set: {len(val_dataset)} images\")\n",
    "print(f\"Testing set: {len(test_dataset)} images\")\n",
    "print(f\"Number of classes: {len(class_to_idx)}\")\n",
    "print(f\"Batch size: {batch_size}\")\n",
    "print(\"Data preparation complete and ready for AlexNet model training!\")\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     print(f\"Training set: {len(train_dataset)} images\")\n",
    "#     print(f\"Validation set: {len(val_dataset)} images\")\n",
    "#     print(f\"Testing set: {len(test_dataset)} images\")\n",
    "#     print(f\"Number of classes: {len(class_to_idx)}\")\n",
    "#     print(f\"Batch size: {batch_size}\")\n",
    "#     print(\"Data preparation complete and ready for AlexNet model training!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loader num_workers: 0\n",
      "Val loader num_workers: 0\n"
     ]
    }
   ],
   "source": [
    "# Add this right after creating your DataLoaders\n",
    "print(f\"Train loader num_workers: {train_loader.num_workers}\")\n",
    "print(f\"Val loader num_workers: {val_loader.num_workers}\")\n",
    "# print(f\"Test loader num_workers: {test_loader.num_workers}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235\n"
     ]
    }
   ],
   "source": [
    "print(len(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.1939,  0.3309,  0.3823,  ..., -0.2684, -0.3541, -0.4739],\n",
      "         [-0.0458,  0.1083,  0.2453,  ..., -0.3541, -0.4397, -0.5253],\n",
      "         [-0.3541, -0.1486,  0.0398,  ..., -0.4739, -0.4911, -0.5253],\n",
      "         ...,\n",
      "         [-1.0733, -0.9705, -0.8849,  ..., -0.6281, -0.6281, -0.6965],\n",
      "         [-0.9705, -0.9363, -0.8849,  ..., -0.3712, -0.6452, -0.5938],\n",
      "         [-1.0048, -0.9534, -0.9020,  ..., -0.1828, -0.4226, -0.3883]],\n",
      "\n",
      "        [[ 0.2402,  0.3803,  0.3978,  ...,  0.1527,  0.1352,  0.1176],\n",
      "         [ 0.1001,  0.2227,  0.2927,  ...,  0.1352,  0.1176,  0.1001],\n",
      "         [-0.0749,  0.0301,  0.1527,  ...,  0.1176,  0.1527,  0.1702],\n",
      "         ...,\n",
      "         [-0.5826, -0.5301, -0.4426,  ..., -0.4251, -0.4776, -0.5301],\n",
      "         [-0.5476, -0.5476, -0.4951,  ..., -0.2500, -0.5301, -0.5126],\n",
      "         [-0.5826, -0.5651, -0.5126,  ..., -0.0574, -0.3025, -0.3025]],\n",
      "\n",
      "        [[-0.4973, -0.3578, -0.3404,  ..., -0.8110, -0.8110, -0.8284],\n",
      "         [-0.6715, -0.5321, -0.4450,  ..., -0.8458, -0.8458, -0.8458],\n",
      "         [-0.8458, -0.7413, -0.6018,  ..., -0.8633, -0.8458, -0.8110],\n",
      "         ...,\n",
      "         [-1.1770, -1.1073, -1.0201,  ..., -1.3513, -1.4036, -1.5081],\n",
      "         [-1.1596, -1.1421, -1.1247,  ..., -1.0724, -1.3513, -1.3513],\n",
      "         [-1.1944, -1.1596, -1.1421,  ..., -0.8807, -1.1247, -1.1421]]])\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset[0][0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
